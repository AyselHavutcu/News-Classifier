{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus.reader import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "import dash_renderer\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.graph_objs as go\n",
    "import re\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = \"C:/Users/HP/Desktop/stajprojesi/usedpickles/best_svc.pickle\"\n",
    "\n",
    "# SVM\n",
    "with open(path_model, 'rb') as data:\n",
    "    svc_model = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. TF-IDF object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tfidf = \"C:/Users/HP/Desktop/stajprojesi/usedpickles/tfidf.pickle\"\n",
    "\n",
    "with open(path_tfidf, 'rb') as data:\n",
    "    tfidf = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Category mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4,\n",
    "    'other':5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Definition of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dailymail\n",
    "\n",
    "def get_news_dailymail():\n",
    "\n",
    "    # url definition\n",
    "    url = \"https://www.dailymail.co.uk\"\n",
    "\n",
    "\n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='linkro-darkred')\n",
    "\n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = url + coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('p', class_='mol-para-with-font')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(body)):\n",
    "            paragraph = body[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        # Removing special characters\n",
    "        final_article = re.sub(\"\\\\xa0\", \"\", final_article)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents  \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "          'Newspaper': 'Daily Mail'})\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "def get_news_skynews():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://news.sky.com/us\"\n",
    "\n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h3', class_=\"sdc-site-tile__headline\")\n",
    "\n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = \"https://news.sky.com\" + coverpage_news[n].find('a', class_='sdc-site-tile__headline-link')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').find('span').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='sdc-article-body sdc-article-body--story sdc-article-body--lead')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'Sky News'})\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "#Hurriyet\n",
    "def get_news_hurriyet():\n",
    "   \n",
    "    #define url\n",
    "    url=\"https://www.hurriyetdailynews.com/\"\n",
    "\n",
    "    #List of news:\n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('div', class_='news')\n",
    "\n",
    "    #print(len(coverpage_news))\n",
    "    #Let's extract the text from the articles:\n",
    "    number_of_articles = 5\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # We need to ignore \"live\" pages since they are not articles\n",
    "        if \"live\" in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(\"https://www.hurriyetdailynews.com/\" + link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='content')\n",
    "        if(len(body) != 0):\n",
    "            x = body[0].find_all('p')\n",
    "        else:\n",
    "            print(\"there is no article content\")\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "        'Newspaper': 'Hurriyet'})\n",
    "   \n",
    "\n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "#sabah\n",
    "def get_news_sabah():\n",
    "\n",
    "    #url\n",
    "    url=\"https://www.dailysabah.com/\"\n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('div', class_='widget_content')\n",
    "\n",
    "    #Let's extract the text from the articles:\n",
    "    number_of_articles = 5\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # We need to ignore \"live\" pages since they are not articles\n",
    "        if \"live\" in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='article_body')\n",
    "        if(len(body) != 0):\n",
    "            x = body[0].find_all('p')\n",
    "        else:\n",
    "            print(\"there is no article content\")\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper':'Sabah'})\n",
    "    \n",
    "    return (df_features,df_show_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "def create_features_from_df(df):\n",
    "    \n",
    "    df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')\n",
    "    \n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()\n",
    "    \n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "        \n",
    "    df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nrows = len(df)\n",
    "    lemmatized_text_list = []\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "        # Save the text and its words into an object\n",
    "        text = df.loc[row]['Content_Parsed_4']\n",
    "        text_words = text.split(\" \")\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "    df['Content_Parsed_5'] = lemmatized_text_list\n",
    "    \n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "        \n",
    "    df = df['Content_Parsed_6']\n",
    "    df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "    \n",
    "    # TF-IDF\n",
    "    features = tfidf.transform(df).toarray()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_category_name(category_id):\n",
    "    for category, id_ in category_codes.items():    \n",
    "        if id_ == category_id:\n",
    "            return category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_features(features):\n",
    "        \n",
    "    # Obtain the highest probability of the predictions for each article\n",
    "    predictions_proba = svc_model.predict_proba(features).max(axis=1)    \n",
    "    \n",
    "    # Predict using the input model\n",
    "    predictions_pre = svc_model.predict(features)\n",
    "\n",
    "    # Replace prediction with 6 if associated cond. probability less than threshold\n",
    "    predictions = []\n",
    "\n",
    "    for prob, cat in zip(predictions_proba, predictions_pre):\n",
    "        if prob > .65:\n",
    "            predictions.append(cat)\n",
    "        else:\n",
    "            predictions.append(5)\n",
    "\n",
    "    # Return result\n",
    "    categories = [get_category_name(x) for x in predictions]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_df(df, categories):\n",
    "    df['Prediction'] = categories\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dash App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "# Colors\n",
    "colors = {\n",
    "    'background': '#ECECEC',  \n",
    "    'text': '#696969',\n",
    "    'titles': '#599ACF',\n",
    "    'blocks': '#F7F7F7',\n",
    "    'graph_background': '#F7F7F7',\n",
    "    'banner': '#C3DCF2'\n",
    "\n",
    "}\n",
    "\n",
    "# Markdown text\n",
    "markdown_text1 = '''\n",
    "\n",
    "This application gathers the latest news from the newspapers **Daily Mail**, **The Guardian** and **Hurriyet(Turkish News Paper)**, predicts their category between **Politics**, **Business**, **Entertainment**, **Sport**, **Tech** and **Other** and then shows a summary.\n",
    "\n",
    "The scraped news are converted into a numeric feature vector with *TF-IDF vectorization*. Then, a *Support Vector Classifier* is applied to predict each category.\n",
    "\n",
    "This app is meant for didactic purposes.\n",
    "\n",
    "Please enter which newspapers would you like to scrape news off and press the **Scrape** button.\n",
    "\n",
    "'''\n",
    "\n",
    "markdown_text2 = '''\n",
    "\n",
    " Created by Aysel Havutcu.\n",
    "\n",
    " *Disclaimer: this app is not under periodic maintenance. A live web-scraping process is carried out every time you run the app, so there may be some crashes due to the failing status of some requests.*\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "app.layout = html.Div(style={'backgroundColor':colors['background']}, children=[\n",
    "    \n",
    "    # Space before title\n",
    "    html.H1(children=' ',\n",
    "            style={'padding': '10px'}\n",
    "           ),\n",
    "    \n",
    "    # Title\n",
    "    html.Div(\n",
    "        [\n",
    "            html.H3(children='News Classification App',\n",
    "                    style={\"margin-bottom\": \"0px\"}\n",
    "                   ),\n",
    "            html.H6(children='A Machine Learning based app')\n",
    "        ],\n",
    "        style={\n",
    "            'textAlign': 'center',\n",
    "            'color': colors['text'],\n",
    "            #'padding': '0px',\n",
    "            'backgroundColor': colors['background']\n",
    "              },\n",
    "        className='banner',\n",
    "            ),\n",
    "    \n",
    "\n",
    "    # Space after title\n",
    "    html.H1(children=' ',\n",
    "            style={'padding': '1px'}),\n",
    "\n",
    "\n",
    "    # Text boxes\n",
    "    html.Div(\n",
    "        [\n",
    "            html.Div(\n",
    "                [\n",
    "                    html.H6(children='What does this app do?',\n",
    "                            style={'color':colors['titles']}),\n",
    "                    \n",
    "                    html.Div(\n",
    "                        [dcc.Markdown(children=markdown_text1),],\n",
    "                        style={'font-size': '12px',\n",
    "                               'color': colors['text']}),\n",
    "                                        \n",
    "                    html.Div(\n",
    "                        [\n",
    "                            dcc.Dropdown(\n",
    "                                options=[\n",
    "                                    {'label': 'Daily Mail', 'value': 'DLM'},\n",
    "                                    {'label': 'Sky News', 'value': 'SKN'},\n",
    "                                    {'label': 'Hurriyet', 'value': 'HRT'},\n",
    "                                    {'label' :'Sabah','value':'SBH'}\n",
    "                                        ],\n",
    "                                value=['DLM'],\n",
    "                                multi=True,\n",
    "                                id='checklist'),\n",
    "                        ],\n",
    "                        style={'font-size': '12px',\n",
    "                               'margin-top': '25px'}),\n",
    "                    \n",
    "                    html.Div([\n",
    "                        html.Button('Scrape', \n",
    "                                    id='submit', \n",
    "                                    type='submit', \n",
    "                                    style={'color': colors['blocks'],\n",
    "                                           'background-color': colors['titles'],\n",
    "                                           'border': 'None'})],\n",
    "                        style={'textAlign': 'center',\n",
    "                               'padding': '20px',\n",
    "                               \"margin-bottom\": \"0px\",\n",
    "                               'color': colors['titles']}),\n",
    "            \n",
    "                    dcc.Loading(id=\"loading-1\", children=[html.Div(id=\"loading-output-1\")], type=\"circle\"),\n",
    "                    \n",
    "                    html.Hr(),\n",
    "                    html.H6(children='Headlines',\n",
    "                            style={'color': colors['titles']}),\n",
    "\n",
    "                    # Headlines\n",
    "                    html.A(id=\"textarea1a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea1b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea2a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea2b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea3a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea3b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea4a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea4b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea5a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea5b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea6a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea6b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea7a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea7b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea8a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea8b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea9a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea9b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea10a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea10b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea11a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea11b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea12a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea12b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea13a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea13b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea14a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea14b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea15a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea15b\", style={'color': colors['text'], 'font-size': '11px'})\n",
    "                                                            \n",
    "                ],\n",
    "                     style={'backgroundColor': colors['blocks'],\n",
    "                            'padding': '20px',\n",
    "                            'border-radius': '5px',\n",
    "                            'box-shadow': '1px 1px 1px #9D9D9D'},\n",
    "                     className='one-half column'),\n",
    "            \n",
    "            html.Div(\n",
    "                [\n",
    "                    html.H6(\"Graphic summary\",\n",
    "                            style={'color': colors['titles']}),\n",
    "\n",
    "                    html.Div([\n",
    "                         dcc.Graph(id='graph1', style={'height': '300px'})\n",
    "                         ],\n",
    "                         style={'backgroundColor': colors['blocks'],\n",
    "                                'padding': '20px'}\n",
    "                    ),\n",
    "                    \n",
    "                    html.Div([\n",
    "                         dcc.Graph(id='graph2', style={'height': '300px'})\n",
    "                         ],\n",
    "                         style={'backgroundColor': colors['blocks'],\n",
    "                                'padding': '20px'}\n",
    "                    )\n",
    "                ],\n",
    "                     style={'backgroundColor': colors['blocks'],\n",
    "                            'padding': '20px',\n",
    "                            'border-radius': '5px',\n",
    "                            'box-shadow': '1px 1px 1px #9D9D9D'},\n",
    "                     className='one-half column')\n",
    "\n",
    "        ],\n",
    "        className=\"row flex-display\",\n",
    "        style={'padding': '20px',\n",
    "               'margin-bottom': '0px'}\n",
    "    ),\n",
    "    \n",
    "        \n",
    "    # Space\n",
    "    html.H1(id='space2', children=' '),\n",
    "        \n",
    "    \n",
    "    # Final paragraph\n",
    "    html.Div(\n",
    "            [dcc.Markdown(children=markdown_text2),],\n",
    "            style={'font-size': '12px',\n",
    "                   'color': colors['text']}),\n",
    "\n",
    "    \n",
    "    # Hidden div inside the app that stores the intermediate value\n",
    "    html.Div(id='intermediate-value', style={'display': 'none'})\n",
    "    \n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [\n",
    "    Output('intermediate-value', 'children'),\n",
    "    Output('loading-1', 'children')\n",
    "    ],\n",
    "    [Input('submit', 'n_clicks')],\n",
    "    [State('checklist', 'value')])\n",
    "def scrape_and_predict(n_clicks, values):\n",
    "            \n",
    "    df_features = pd.DataFrame()\n",
    "    df_show_info = pd.DataFrame()\n",
    "    \n",
    "    if 'DLM' in values:\n",
    "        # Get the scraped dataframes\n",
    "        df_features = df_features.append(get_news_dailymail()[0])\n",
    "        df_show_info = df_show_info.append(get_news_dailymail()[1])\n",
    "    \n",
    "    if 'SKN' in values:\n",
    "        df_features = df_features.append(get_news_skynews()[0])\n",
    "        df_show_info = df_show_info.append(get_news_skynews()[1])\n",
    "        \n",
    "    if 'HRT' in values:\n",
    "        df_features = df_features.append(get_news_hurriyet()[0])\n",
    "        df_show_info = df_show_info.append(get_news_hurriyet()[1])\n",
    "        \n",
    "    if 'SBH' in values:\n",
    "        df_features = df_features.append(get_news_sabah()[0])\n",
    "        df_show_info = df_show_info.append(get_news_sabah()[1])\n",
    "\n",
    "\n",
    "    df_features = df_features.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # Create features\n",
    "    features = create_features_from_df(df_features)\n",
    "    # Predict\n",
    "    predictions = predict_from_features(features)\n",
    "    # Put into dataset\n",
    "    df = complete_df(df_show_info, predictions)\n",
    "    # df.to_csv('Tableau Teaser/df_tableau.csv', sep='^')  # export to csv to work out an example in Tableau\n",
    "    \n",
    "    return df.to_json(date_format='iso', orient='split'), ' '\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph1', 'figure'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_barchart(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    # Create a summary df\n",
    "    df_sum = df.groupby(['Newspaper', 'Prediction']).count()['Article Title']\n",
    "\n",
    "    # Create x and y arrays for the bar plot for every newspaper\n",
    "    if 'Daily Mail' in df_sum.index:\n",
    "    \n",
    "        df_sum_epe = df_sum['Daily Mail']\n",
    "        x_epe = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_epe = [[df_sum_epe['politics'] if 'politics' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['business'] if 'business' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['entertainment'] if 'entertainment' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['sport'] if 'sport' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['tech'] if 'tech' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['other'] if 'other' in df_sum_epe.index else 0][0]]   \n",
    "    else:\n",
    "        x_epe = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_epe = [0,0,0,0,0,0]\n",
    "    \n",
    "    if 'Sky News' in df_sum.index:\n",
    "        \n",
    "        df_sum_thg = df_sum['Sky News']\n",
    "        x_thg = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_thg = [[df_sum_thg['politics'] if 'politics' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['business'] if 'business' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['entertainment'] if 'entertainment' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['sport'] if 'sport' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['tech'] if 'tech' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['other'] if 'other' in df_sum_thg.index else 0][0]]   \n",
    "    else:\n",
    "        x_thg = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_thg = [0,0,0,0,0,0]\n",
    "\n",
    "    if 'Hurriyet' in df_sum.index:\n",
    "    \n",
    "        df_sum_skn = df_sum['Hurriyet']\n",
    "        x_skn = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_skn = [[df_sum_skn['politics'] if 'politics' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['business'] if 'business' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['entertainment'] if 'entertainment' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['sport'] if 'sport' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['tech'] if 'tech' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['other'] if 'other' in df_sum_skn.index else 0][0]]   \n",
    "\n",
    "    else:\n",
    "        x_skn = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_skn = [0,0,0,0,0,0]\n",
    "        \n",
    "    if 'Sabah' in df_sum.index:\n",
    "    \n",
    "        df_sum_sbh = df_sum['Sabah']\n",
    "        x_sbh = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_sbh = [[df_sum_sbh['politics'] if 'politics' in df_sum_sbh.index else 0][0],\n",
    "                [df_sum_sbh['business'] if 'business' in df_sum_sbh.index else 0][0],\n",
    "                [df_sum_sbh['entertainment'] if 'entertainment' in df_sum_sbh.index else 0][0],\n",
    "                [df_sum_sbh['sport'] if 'sport' in df_sum_sbh.index else 0][0],\n",
    "                [df_sum_sbh['tech'] if 'tech' in df_sum_sbh.index else 0][0],\n",
    "                [df_sum_sbh['other'] if 'other' in df_sum_sbh.index else 0][0]]   \n",
    "\n",
    "    else:\n",
    "        x_sbh = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_sbh = [0,0,0,0,0,0]\n",
    "\n",
    "    # Create plotly figure\n",
    "    figure = {\n",
    "        'data': [\n",
    "            {'x': x_epe, 'y':y_epe, 'type': 'bar', 'name': 'Daily Mail', 'marker': {'color': 'rgb(62, 137, 195)'}},\n",
    "            {'x': x_thg, 'y':y_thg, 'type': 'bar', 'name': 'Sky News', 'marker': {'color': 'rgb(167, 203, 232)'}},\n",
    "            {'x': x_skn, 'y':y_skn, 'type': 'bar', 'name': 'Hurriyet', 'marker': {'color': 'rgb(197, 223, 242)'}},\n",
    "            {'x': x_sbh, 'y':y_sbh, 'type': 'bar', 'name': 'Sabah', 'marker': {'color': 'rgb(197, 223, 242)'}}\n",
    "        ],\n",
    "        'layout': {\n",
    "            'title': 'Number of news articles by newspaper',\n",
    "            'plot_bgcolor': colors['graph_background'],\n",
    "            'paper_bgcolor': colors['graph_background'],\n",
    "            'font': {\n",
    "                    'color': colors['text'],\n",
    "                    'size': '10'\n",
    "            },\n",
    "            'barmode': 'stack'\n",
    "            \n",
    "        }   \n",
    "    }\n",
    "\n",
    "    return figure\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph2', 'figure'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_piechart(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    # Create a summary df\n",
    "    df_sum = df['Prediction'].value_counts()\n",
    "\n",
    "    # Create x and y arrays for the bar plot\n",
    "    x = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "    y = [[df_sum['politics'] if 'politics' in df_sum.index else 0][0],\n",
    "         [df_sum['business'] if 'business' in df_sum.index else 0][0],\n",
    "         [df_sum['entertainment'] if 'entertainment' in df_sum.index else 0][0],\n",
    "         [df_sum['sport'] if 'sport' in df_sum.index else 0][0],\n",
    "         [df_sum['tech'] if 'tech' in df_sum.index else 0][0],\n",
    "         [df_sum['other'] if 'other' in df_sum.index else 0][0]]\n",
    "    \n",
    "    # Create plotly figure\n",
    "    figure = {\n",
    "        'data': [\n",
    "            {'values': y,\n",
    "             'labels': x, \n",
    "             'type': 'pie',\n",
    "             'hole': .4,\n",
    "             'name': '% of news articles',\n",
    "             'marker': {'colors': ['rgb(62, 137, 195)',\n",
    "                                   'rgb(167, 203, 232)',\n",
    "                                   'rgb(197, 223, 242)',\n",
    "                                   'rgb(51, 113, 159)',\n",
    "                                   'rgb(64, 111, 146)',\n",
    "                                   'rgb(31, 84, 132)']},\n",
    "\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        'layout': {\n",
    "            'title': 'News articles by newspaper',\n",
    "            'plot_bgcolor': colors['graph_background'],\n",
    "            'paper_bgcolor': colors['graph_background'],\n",
    "            'font': {\n",
    "                    'color': colors['text'],\n",
    "                    'size': '10'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return figure\n",
    "    \n",
    "    \n",
    "@app.callback(\n",
    "    [\n",
    "    Output('textarea1a', 'href'),\n",
    "    Output('textarea1a', 'children'),\n",
    "    Output('textarea1b', 'children'),\n",
    "    Output('textarea2a', 'href'),\n",
    "    Output('textarea2a', 'children'),\n",
    "    Output('textarea2b', 'children'),\n",
    "    Output('textarea3a', 'href'),\n",
    "    Output('textarea3a', 'children'),\n",
    "    Output('textarea3b', 'children'),\n",
    "    Output('textarea4a', 'href'),\n",
    "    Output('textarea4a', 'children'),\n",
    "    Output('textarea4b', 'children'),\n",
    "    Output('textarea5a', 'href'),\n",
    "    Output('textarea5a', 'children'),\n",
    "    Output('textarea5b', 'children'),\n",
    "    Output('textarea6a', 'href'),\n",
    "    Output('textarea6a', 'children'),\n",
    "    Output('textarea6b', 'children'),\n",
    "    Output('textarea7a', 'href'),\n",
    "    Output('textarea7a', 'children'),\n",
    "    Output('textarea7b', 'children'),\n",
    "    Output('textarea8a', 'href'),\n",
    "    Output('textarea8a', 'children'),\n",
    "    Output('textarea8b', 'children'),\n",
    "    Output('textarea9a', 'href'),\n",
    "    Output('textarea9a', 'children'),\n",
    "    Output('textarea9b', 'children'),\n",
    "    Output('textarea10a', 'href'),\n",
    "    Output('textarea10a', 'children'),\n",
    "    Output('textarea10b', 'children'),\n",
    "    Output('textarea11a', 'href'),\n",
    "    Output('textarea11a', 'children'),\n",
    "    Output('textarea11b', 'children'),\n",
    "    Output('textarea12a', 'href'),\n",
    "    Output('textarea12a', 'children'),\n",
    "    Output('textarea12b', 'children'),\n",
    "    Output('textarea13a', 'href'),\n",
    "    Output('textarea13a', 'children'),\n",
    "    Output('textarea13b', 'children'),\n",
    "    Output('textarea14a', 'href'),\n",
    "    Output('textarea14a', 'children'),\n",
    "    Output('textarea14b', 'children'),\n",
    "    Output('textarea15a', 'href'),\n",
    "    Output('textarea15a', 'children'),\n",
    "    Output('textarea15b', 'children')\n",
    "    ],\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_textarea1(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    texts = []\n",
    "    links = []\n",
    "    preds_newsp = []\n",
    "    \n",
    "    for article in range(len(df)):\n",
    "        texts.append(df.iloc[article]['Article Title'])\n",
    "        links.append(df.iloc[article]['Article Link'])\n",
    "        preds_newsp.append((df.iloc[article]['Prediction'].capitalize()) + ', ' + (df.iloc[article]['Newspaper']))\n",
    "\n",
    "    while (len(texts) < 16):\n",
    "        texts.append(None)\n",
    "        links.append(None)\n",
    "        preds_newsp.append(None)\n",
    "    \n",
    "    return \\\n",
    "        links[0], texts[0], preds_newsp[0],\\\n",
    "        links[1], texts[1], preds_newsp[1],\\\n",
    "        links[2], texts[2], preds_newsp[2],\\\n",
    "        links[3], texts[3], preds_newsp[3],\\\n",
    "        links[4], texts[4], preds_newsp[4],\\\n",
    "        links[5], texts[5], preds_newsp[5],\\\n",
    "        links[6], texts[6], preds_newsp[6],\\\n",
    "        links[7], texts[7], preds_newsp[7],\\\n",
    "        links[8], texts[8], preds_newsp[8],\\\n",
    "        links[9], texts[9], preds_newsp[9],\\\n",
    "        links[10], texts[10], preds_newsp[10],\\\n",
    "        links[11], texts[11], preds_newsp[11],\\\n",
    "        links[12], texts[12], preds_newsp[12],\\\n",
    "        links[13], texts[13], preds_newsp[13],\\\n",
    "        links[14], texts[14], preds_newsp[14]\n",
    "           \n",
    "    \n",
    "    \n",
    "# Loading CSS\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/chriddyp/pen/bWLwgP.css\"})\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/chriddyp/pen/brPBPO.css\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
